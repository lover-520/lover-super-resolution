# 图像超分基础知识

地面采样距离（GSD），也称地面采样间隔，在摄影测量与遥感中，指数字影像中用地面距离单位表示的像素大小，单位为米/像素，地面采样距离是衡量影像分辨率的重要指标。
对于数字航空影像或者航天遥感影像而言，影像的分辨率通常指空间分辨率（IFOV）或者地面采样距离GSD。GSD为地面采样距离，一般以一个像素所代表的地面大小来表示（米/像素）。如GSD为5厘米/像素，代表一个像素表示实际5厘米*5厘米。
比例尺1：500，代表地图上1米表示实际500米；
英寸，丈量屏幕的长度、宽度的单位，1英寸=0.0254米。
DPI，每英寸包含的像素，一般打印出图300是300dpi，即一英寸中包含300个像素。
重采样并不会改变影像的GSD值，所以将高分辨率的影像重采样为低分辨率之后的影像仍然会比之前的更为清晰，这仅仅只是改变了里面的像元大小。

## 超分数据集

DIV2K、Set5、Set14

## 超分问题质量评估指标

峰值信噪比（Peak signal-to-noise ratio, PSNR）
关心图像像素值之间的差异，并不能很好地代表感知质量；

结构相似度SSIM
从亮度、对比度和结构三个方面衡量图像之间的结构相似性

## 超分网络框架

基于所采用的上采用操作及其在模型中的位置可分为：

Pre-upsampling Super-resolution
先采用传统的上采样方法（双线性插值法、双三次插值法）将图像直接上采样到高分辨率，再利用深度神经网络进行微调。（LR图像被使用传统方法上采样到粗糙的HR图像，然后应用CNN到这些粗糙的HR图像上面，用于建立高质量细节的图像，这效果肯定不好吧，前面上采样的参数已经确定了，上采样的效果将直接影响到后面深度学习的效果）
代表网络：SRCNN、VDSR、DRCN、DRRN
优点：最困难的上采样操作交给了传统方法解决，深度神经网络只需要对粗糙的生成图片进行精调，大大降低了训练的难度；可以得到任意大小的HR图片，处理任意维度的LR图片，通过调节超分任务的scale参数即可；
缺点：传统的上采样操作会带来很多副作用，比如噪声放大和模糊的问题，增加了不必要的计算成本，并且不提供用于重建高分辨率图像的附件高频信息；

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled.png)

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%201.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%201.png)

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%202.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%202.png)

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%203.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%203.png)

Post-upsampling Super-resolution
为了解决Pre-upsampling Super-resolution的计算成本太高的问题，Post-upsampling将主要的计算任务放在低分辨率的位置，将上采样的操作放在模型的最后方，并将上采样的操作换成可学习的层。
优点：大量的计算在低分辨率的空间中进行，可以提高计算的效率；
缺点：上采样仅在一个步骤中执行，如果当放大倍数比较大时，比如4或者8，极大地增加了学习难度；每一种放大因子都需要训练一个独立的模型，无法满足多尺度超分任务的需求；

Progressive upsampling Super-resolution
针对Post-upsampling Super-resolution中上采样存在的问题，开始尝试渐进式的进行上采样。基于深度卷积神经网络的级联，逐步重建高分辨率图像；模型包括多个不同的阶段，在每个阶段，图像被上采样一次得到更高的分辨率，并且在上一次CNN的基础上进行微调；
优点：将复杂的上采样任务分解为几步进行，渐进式的操作减少了学习的困难，尤其是在上采样比较多的情况；
缺点：多阶段的上采样导致模型的设计复杂，存在多种不同的情况，以及训练稳定性的问题；需要更加复杂和先进的训练策略进行指导；

Iterative Up-and-down Sampling Super-resolution
为了更好地捕捉LR-HR图像对的相互依存性，提出了使用back-projection的操作，迭代地在上采样之后进行back-projection操作并精调（refinement）通过计算reconstruction error来调整HR图像；

## 常见的上采样方法

传统的上采样方法：

1. 最近邻插值：每个待插值的位置选择最相邻的像素值，而不考虑其他像素；
2. 双线性插值：首先对图像的一个轴进行线性插值，然后在另一轴上执行；
3. 双三次插值：首先对图像的一个轴进行三次插值，然后在另一轴上执行；

可学习的上采样方法：

1. 转置卷积层：也称反卷积
2. 亚像素层sub-pixel convolution: 思想是通过卷积操作产生大量的channel，然后通过reshape操作得到最终的输出；
3. Meta upscale module: 以往的模型需要预先定义缩放因子，针对不同的因子训练不同的上采样模块，效率低下，不满足实际需求；Meta upscale模块基于元学习解决任意比例因子的SR。

## 常用网络设计

## 常用损失函数

1. 像素损失：最简单的一类损失函数，其中生成的图像中的每个像素都直接与ground-truth图像中的每个像素进行比较，例如L1损失或者L2损失；PSNR指标与像素损失高度相关，因此最小化像素损失可以直接最大化PSNR指标，但是像素损失无法考虑图像质量，模型输出往往在感知上达不到令人满意的效果；
2. 内容损失：基于图像的感知质量来评估图像质量，一种方法是比较生成的图像和ground-truth图像的高层特征；
3. 纹理损失：使生成的图像具有与ground-truth图像相同的样式（纹理、颜色、对比度等），图像的纹理被定义为不同特征通道之间的相关性；
4. Total Variation损失：抑制生成图像中的噪声；
5. 对抗损失：

## 网络

### ***SRCNN  TPAMI’2015***

[](http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepresolution.pdf)

先经过双三次插值上采样到高分辨率图像的大小，然后包含三层神经网络：
1. 第一层网络用于特征提取；
2. 第二层网络用于非线性映射，降低分辨率图片映射到高分辨率，卷积核大小为1*1；
3. 第三层网络用于图像重构，用户恢复细节，得到清晰的高分辨率图像；

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%204.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%204.png)

---

### ***FSRCNN  2016***

[Accelerating the Super-Resolution Convolutional Neural Network](https://arxiv.org/abs/1608.00367)

不使用传统的双三次插值，而是直接对分辨率图像进行特征提取，然后经过一系列的卷积操作，最后经过一个反卷积层的，得到高分辨率的图片。优点在于缩小图片可以降低训练的时间，同时，如果需要得到不同分辨率的图片，单独训练最后的反卷积层即可；

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%205.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%205.png)

---

### ***ESPCN  CVPR2016***

[](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shi_Real-Time_Single_Image_CVPR_2016_paper.pdf)

同样地从上采样的角度对模型进行改进，提出了亚像素卷积层的概念。在超分领域中，对于低分辨率而言，恢复出的高分辨率图片结果是不能确定的，有很多种可能。对低分辨率的图片上，每一个像素能够表示的范围有限，在成像时，由于感光元件的能力有限，每个像素只能表示附近的颜色，宏观上两个相邻像素是连在一起的，实际上它们之间还有无数微小的东西存在，这些存在于两个实际物理像素之间的像素，被称为亚像素。亚像素是实际存在的，只是缺少强大的传感器将其检测出来，只能通过算法近似计算。

该网络中其余部分并无较大差别，仅最后的上采样操作为亚像素卷积层的方法，得到r^2个通道，最后重新排列。

这种亚像素卷积的方式在PyTorch中实现为PixelShuffle，在TensorFlow中实现为depthtospace；

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%206.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%206.png)

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%207.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%207.png)

---

### ***DRCN  CVPR2016***

[](https://openaccess.thecvf.com/content_cvpr_2016/papers/Kim_Deeply-Recursive_Convolutional_Network_CVPR_2016_paper.pdf)

将递归神经网络模型应用到超分辨率领域中（虽然不是第一个将递归神经网络模型应用到超分中，但却是第一个递归深度这么深的），递归深度太深容易导致模型崩溃的问题。网络中的递归部分就是一般的网络层堆叠在一起，只不过这些网络中的卷积权重是共享的，这就大大减小了网络的参数量。当递归次数增多的时候，递归网络模型就会出现出现两种问题：容易出现梯度消失或者梯度爆炸的问题；类似于RNN，输出与输出之间有较长的依赖关系，这种依赖关系导致模型很难获得图像中较好的细节部分。因此，作者针对每种问题采用了解决方案：提出了递归监督的方法，即网络中的每一个递归层的递归映射都被用来重建HR图片；采用残差学习中的跳跃连接解决输出与输入之间的长期依赖的关系；

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%208.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%208.png)

---

### ***VDSR  CVPR2016***

[](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper.pdf)

针对SRCNN中存在的问题：
1. 感受野过小：在卷积核尺寸一定的情况下，网络层数过浅导致生成的图片感受野过小；更深的网络结构能够带来更大的感受野，能够利用更多的上下文信息；
2. 在加深网络深度的同时，势必会减缓网络的收敛速率甚至会导致网络无法收敛，因此作者提出使用残差学习和大学习率；LR和HR图片共享了很多基本信息，即LR图片中的低频信息，所以我们只需要学习两者之间的差值即可；
3. SRCNN需要设计不同的网络生成不同的模型来解决不同尺度的放大问题，作者提出用一个网络训练不同尺度的图片来得到一个模型；

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%209.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%209.png)

---

### ***RED  2016***

[](https://proceedings.neurips.cc/paper/2016/file/0ed9422357395a0d4879191c66f4faa2-Paper.pdf?fbclid=IwAR0PD0FrbYggQX28MFcMK4vHMMsMm-duhfXVoGBP9vP39muTtFb09IEgBX4)

网络结构比较简单，一种类似于UNet的思想，在当时也算是比较经典的模型了，较好地解决了梯度消失的问题；

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2010.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2010.png)

---

### ***DRRN  CVPR2017***

[](https://openaccess.thecvf.com/content_cvpr_2017/papers/Tai_Image_Super-Resolution_via_CVPR_2017_paper.pdf)

从下面的网络模型中不难看出，这篇文章主要在于将VDSR中的残差学习思路和DRCN中的递归神经网络的思路集合到一起，提出了局部残差连接和残差学习的递归方法；（需要注意的是在DRRN的网络结构图中，绿色框中的网络层共享权重，红色框内的网络层共享权重）

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2011.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2011.png)

---

### ***LapSRN  CVPR2017***

[](https://openaccess.thecvf.com/content_cvpr_2017/papers/Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper.pdf)

针对之前的超分辨率模型的问题进行了分析：
1. 使用预定义好的上采样操作，从SRCNN开始，就一直使用双三次插值的方法将低分辨率的图片输入插值到高分辨率的输出上；而后面提出的FSRCNN和ESPCN分别使用了反卷积和亚像素卷积进行加速提高准确率，但是输入分辨率太高，约束着模型结构的容量，无法学习更为复杂的映射关系；（这种插值的上采样的方式也有优点，在于可以控制上采样的倍数）
2. 大多数模型使用L2 Loss作为损失函数，会导致最后的重建结果过于平滑，影响视觉质量；
3. 大部分模型一次只能重建一个倍数的超分结果，无法生成多个分辨率下的中间超分结果；

于是提出了如下的拉普拉斯金字塔网络结构：
包括两个分支：特征提取和图像重建；（红色箭头→卷积，蓝色箭头→反卷积，绿色箭头→加法操作，橘色箭头→递归操作）
特征提取中，每一个反卷积层进行尺度为2的上采样，输出层连接两个不同层，向右的箭头表示继续上采样，向下往图像重建的方向的箭头表示每次上采样到一定程度时，将学习到的残差结果输出，得到对应的重建图像；
图像重建中，输入图像通过反卷积进行尺度为2的上采样，使用双线性内核初始化这个层，并让它于其他层联合优化；上采样的图像和从特征提取中得到的预测残差图像联合，产生一个高分辨率的输出图像；

与之前的CNN方法不同在于：
1. 通过卷积操作和反卷积将残差学习和上采样联合起来，利用所学习到的上采样filter，能够有效抑制双三次插值引起的重建伪影的问题；
2. 使用鲁棒性更好的Charbonnier损失函数代替L2损失函数优化网络，可以有效地处理异常值；
3. LapSRN逐步重建HR图像，相同的模型可以用于不同尺度因子的应用，也就是可以适用于不同倍数的方法；（其实也就适用于2的幂次）

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2012.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2012.png)

---

### ***SRDenseNet  CVPR2017***

将DenseNet的结构引用到超分辨率中，网络可分为四部分：
* 学习低层特征的卷积层；
* 学习高层特征的DenseNet block；
* 学习上采样的反卷积层；
* 生成HR图像的重建层；
网络结构有三种形式：
* 只在dense block中使用密集连接，密集块的高层特征作为反卷积层的输入；
* 添加Skip Connection，将低层特征与高层密集块的特征连接起来作为反卷积层的输入；
* 添加Densely Skip Connection，将每一个dense block的输出都连接起来，作为反卷积层的输入；（由于这样计算开销太大，之后添加了一个1*1的卷积层减少特征数量，被称为Bottleneck layer）
毫无疑问，最后效果c>b>a

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2013.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2013.png)

---

### ***SRGAN  CVPR2017***

[](https://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf)

第一篇将GAN网络应用于超分领域；
针对以往的模型评价指标最大化峰值信噪比，等价于最小化与GT图像的均方重建误差MSE，这会导致高频细节的丢失，图像过于平滑（或者说是模糊），超分图像的准确性与人的期望值不符。
作者提出的SRGAN网络结构：
生成网络，backbonne为SRResNet，使用了残差块和跳跃连接，在残差块结束之后，利用2个亚像素卷积增大尺寸；
辨别网络，8次卷积操作+2个全连接层+sigmoid的VGG网络结构，
损失函数：G_Loss, D_Loss
G_loss包含内容损失和对抗损失，内容损失在MSR损失的基础上，加上了VGG网络的预训练损失（VGG网络中第i层第j个卷积核输出的feature map，经过激活之后，的MSE损失）；

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2014.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2014.png)

---

### ***EDSR  CVPR2017***

[](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.pdf)

文章中谈到以往的模型存在的问题：
1. 网络模型的效果会收到网络模型细微的影响，不同的初始化和训练方法会得到不同的效果；
2. 现有的SR算法把不同的缩放因子的超分辨率问题当成独立的问题，需要独立训练不同尺度的模型，没有充分考虑到不同缩放倍数之间的相互关系；
3. 原始的ResNet最初用于high-level的特征提取，直接应用于SR中的关于low-level的特征提取效果并不是最优的；
于是，EDSR去掉了residual block中的BN层，选用更为合适的L1 Loss；
左：单一尺度下，EDSR；右：多尺度下，MDSR

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2015.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2015.png)

![%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2016.png](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2016.png)

---

### ***ESRGAN  ECCVW2018***

[](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf)

SRGAN在SR领域上取得了较好的效果，但生成的图像与GT图像相比，放大之后仍有较大的伪影，因此ESRGAN从网络结构、损失两个方面进行了一系列改进：
1. 去掉BN层；BN层被证明不适用于超分网络；
2. 使用残差模块：去掉BN层导致网络难以收敛，因此在生成网络种使用残差模块；
3. GAN网络改为使用Relativistic average GAN(RaGAN)；
4. 感知损失函数使用激活前的VGG特征，能够提供更尖锐的边缘和更符合视觉效果的感知；
网络主体仍然是采用SRGAN种的SRResNet结构，只是将里面的模块换成了提出的RRDB
判别器的更改为使用相对判别器：
以往的标准判别器判断是真图像为1，而相对判别器判断生成图像与真正图像的差值；

![Untitled](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2017.png)

![Untitled](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2018.png)

---

### ***RDN  CVPR2018***

[](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Residual_Dense_Network_CVPR_2018_paper.pdf)

文章从如何更好地使用LR图像中的信息着手，将残差结构与密集网络结构进行了结合，主要包含以下几点：

![Untitled](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2019.png)

1. shallow feature extraction net(SFENet)，左侧两个蓝色卷积层，提取浅层特征；
2. residual dense blocks(RDBs) 残差密集连接网络，中间D个RDB；

![Untitled](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2020.png)

RDB模块主要将残差结构与密集块连接起来；

![Untitled](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2021.png)

一个RDB结构包括：

- CM（Contiguous Memory，近邻记忆），一个RDB里面有C个Conv+ReLU的结构，那么会将前一个RDB的输出发送个每一个conv；
- LFF（Local Feature Fusion，局部特征融合），将前一个RDB的状态与当前所有的conv串联起来，1*1卷积减少通道数量；
- LRL（Local Residual Learning，局部残差学习），也就是下面那根红色的线；
1. Dense Feature Fusion(DFF)，密集特征融合，分为以下两部分：
    1. global feature fusion（gff），全局特征融合，将所有RDB提取到的特征进行融合，得到全局特征；
    2. global residual learning(grl)，全局残差学习，也就是浅层特征加上所有提取到的RDB特征；
2. Up-Sampling Net(UPNet)，上采样网络，使用ESPCN的结构，亚像素卷积；

---

### ***EDVR  CVPR2019***

[](https://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Wang_EDVR_Video_Restoration_With_Enhanced_Deformable_Convolutional_Networks_CVPRW_2019_paper.pdf)

EDVR模型用于视频超分辨率的任务：给定2N+1个连续帧，定义中间帧作为参考帧，其他帧作为相邻帧。视频重建的目的在于得到一个与GT帧足够相似的高质量参考帧输出；

![Untitled](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2022.png)

相邻帧通过PCD模块与参考帧进行对齐，TSA融合模块融合不同帧的图像信息，融合后的特征传到重建模块。重建模块是连续的残差块，可以容易地被其他单一图像超分的高级模块替换，在网络末端添加上采样模块，最后预测的输出帧GT图像。网络主要包括以下的模块：

1. PreDeblur Module，对每帧图像进行预去模糊；
2. PCD Align Module，
3. TSA Fusion Module，
4. Reconstruction Module，

---

### ***ToFlow  IJCV2019***

视频超分；

---

### ***DIC  CVPR2020***

[](https://openaccess.thecvf.com/content_CVPR_2020/papers/Ma_Deep_Face_Super-Resolution_With_Iterative_Collaboration_Between_Attentive_Recovery_and_CVPR_2020_paper.pdf)

用于人脸超分辨率领域，主要针对先验信息只使用低分辨率图像获取，导致不准确的问题；

网络结构主要包括：low-resolution feature extractor G1, recursive block GR, high resolution generator layers G2

![Untitled](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2023.png)

---

### ***TDAN  CVPR2020***

[](https://openaccess.thecvf.com/content_CVPR_2020/papers/Tian_TDAN_Temporally-Deformable_Alignment_Network_for_Video_Super-Resolution_CVPR_2020_paper.pdf)

视频超分

---

### ***TTSR  CVPR2020***

[](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_Texture_Transformer_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf)

超分领域包括单图像超分辨率研究SISR和基于参考的图像超分辨率研究RefSR。RefSR从给定的参考图像中传输HR纹理以产生视觉上令人愉悦的效果。该文章将Transformer应用到图像超分辨率研究中，（没看懂，后面再补充吧）

![Untitled](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2024.png)

---

### ***LIIF  CVPR2021***

[](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Learning_Continuous_Image_Representation_With_Local_Implicit_Image_Function_CVPR_2021_paper.pdf)

How to represent a image?

作者等人希望学习一种图像的连续表达，受3D重建的隐式函数的启发，提出了一种Local Implicit Image Function(LIIF)，它采用图像坐标以及2D深度特征作为输入，直接预测给定位置上的RGB输出。由于坐标的连续性，LIIF可以表示成任意分辨率形式。

局部隐函数将2D特征均匀地扩散到2D连续图像域；前面可以采用现有超分网络的特征提取部分，比如EDSR、RDN等；

![Untitled](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2025.png)

---

### ***BasicVSR  CVPR2021***

[](https://openaccess.thecvf.com/content/CVPR2021/papers/Chan_BasicVSR_The_Search_for_Essential_Components_in_Video_Super-Resolution_and_CVPR_2021_paper.pdf)

视频超分

---

### ***IconVSR  CVPR2021***

视频超分

---

### ***GLEAN  CVPR2021***

[](https://openaccess.thecvf.com/content/CVPR2021/papers/Chan_GLEAN_Generative_Latent_Bank_for_Large-Factor_Image_Super-Resolution_CVPR_2021_paper.pdf)

[http://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-cat-config-f-official_20210327_172444-15bc485b.pth](http://download.openmmlab.com/mmgen/stylegan2/'%20%20%20%20%20%20%20%20%20%20%20%20%20'official_weights/stylegan2-cat-config-f-official_20210327'%20%20%20%20%20%20%20%20%20%20%20%20%20'_172444-15bc485b.pth)

![Untitled](%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2097bd2/Untitled%2026.png)